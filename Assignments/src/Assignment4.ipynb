{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: SentimentAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv',encoding = \"ISO-8859-1\", names=['polarity', 'id','time', 'query','user', 'text'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your assignment is to perform sentiment analysis on the tweets. You can use one of two approaches to analyze the data:\n",
    "\n",
    "1. Using a machine learning classifier such as a Naive Bayes Classifier. Select 15,000 tweets randomly from each of the 3 categories (0 = negative, 2 = neutral, 4 = positive)to build your training dataset. Select 3000 tweets randomly from the 3 categories to build your test set. Make sure there is no overlap between the training and test sets. Use the training data to train your model and the test data to evaluate the performance of your model. Report the Precision, Recall, and F-1 score of the model. \n",
    "\n",
    "2. Using a lexicon based approach. You can use Textblob or any other Python implementation to compute sentiment for each tweet. Select 15,000 tweets randomly from each of the 3 categories (0 = negative, 2 = neutral, 4 = positive) to build a dataset. Predict the sentiment for these tweets using your model and compare the model's predicted sentiment to the actual sentiment reported in the data (Column 0). Report the Precision, Recall, and F-1 score of the model.\n",
    "\n",
    "Submit an ipynb file along with a report discussing your results.\n",
    "\n",
    "\n",
    "Useful links:\n",
    "1. https://www.dataquest.io/blog/naive-bayes-tutorial/\n",
    "2. https://towardsdatascience.com/creating-the-twitter-sentiment-analysis-program-in-python-with-naive-bayes-classification-672e5589a7ed\n",
    "3. https://triton.ml/blog/sentiment-analysis \n",
    "4. https://medium.freecodecamp.org/how-to-build-a-twitter-sentiments-analyzer-in-python-using-textblob-948e1e8aae14\n",
    "5. https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity          id                          time     query  \\\n",
       "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    800000\n",
      "0    800000\n",
      "Name: polarity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values are equally split 50% positive and 50% negative, but there are no neutral tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split, Sample Data, Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)\n",
    "    #text = re.sub('@[^\\s]+','USER', text)\n",
    "    text = text.lower()    \n",
    "    #text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    text = re.sub(' +',' ', text)\n",
    "    return text.strip()\n",
    "def get_top_n_words(words, n ,stopwords):\n",
    "    word_count_vector = CountVectorizer(max_df=0.95,stop_words=stopwords).fit(words)\n",
    "    bag_of_words = word_count_vector.transform(words)\n",
    "    word_sums = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, word_sums[0, idx]) for word, idx in word_count_vector.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    " \n",
    "def get_stop_words(stop_file_path):\n",
    "#read in stop list words\n",
    "#https://gist.github.com/CristhianBoujon/c719ba2287a630a6d3821d37a9608ac8/cd8308e5ab8ceae3d13c363a67c154c83e560926\n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "    return frozenset(stop_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text']= [preprocess_text(t) for t in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'].head(3)\n",
    "positivetweets = df[df['polarity'] == 4].sample((int)(45000/2), replace=False)\n",
    "negativetweets = df[df['polarity'] == 0].sample((int)(45000/2),replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wingo\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('good', 1742), ('url', 1444), ('day', 1351), ('love', 1335), ('quot', 1289), ('lol', 1054), ('time', 876), ('today', 843), ('amp', 757), ('great', 690), ('ll', 670), ('back', 653), ('night', 645), ('happy', 616), ('haha', 607), ('morning', 586), ('im', 567), ('hope', 546), ('twitter', 520), ('don', 514), ('work', 503), ('fun', 502), ('nice', 494), ('home', 436), ('tomorrow', 432)]\n"
     ]
    }
   ],
   "source": [
    "#get sample stopwords file\n",
    "stopwords=get_stop_words(\"stopwords_en.txt\")\n",
    "#processed positive tweets text\n",
    "docs=positivetweets['processed_text'].tolist()\n",
    "most_common_positive=get_top_n_words(docs,25,stopwords)\n",
    "print(most_common_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wingo\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aahh', 'aarrgghh', 'abt', 'accent', 'accented', 'accents', 'account', 'accounts', 'acne', 'activities', 'activity', 'ad', 'add', 'added', 'adding', 'adds', 'admission', 'admissions', 'ads', 'afaik', 'affiliate', 'affiliates', 'affirmation', 'affirmations', 'aft', 'afternoon', 'ago', 'ahead', 'ain', 'aint', 'aircon', 'album', 'albums', 'allergies', 'allergy', 'allow', 'allowed', 'allows', 'alot', 'am', 'angry', 'announcement', 'announcements', 'annoy', 'annoyed', 'annoys', 'anycase', 'anymore', 'app', 'apparently', 'approve', 'approved', 'approves', 'apps', 'april', 'area', 'areas', 'argh', 'arrive', 'arrived', 'arrives', 'article', 'articles', 'asia', 'asian', 'ask', 'asked', 'asks', 'ass', 'asses', 'ate', 'attempt', 'attempting', 'attempts', 'attend', 'attended', 'attends', 'august', 'auto', 'autoindustry', 'awesome', 'babeh', 'babies', 'baby', 'back', 'backed', 'bad', 'bag', 'bags', 'bai', 'balance', 'bank', 'banks', 'based', 'bcos', 'bcoz', 'bday', 'bed', 'bedroom', 'belong', 'belonged', 'belongs', 'big', 'bigger', 'biggest', 'billion', 'billons', 'birthday', 'birthdays', 'bit', 'biz', 'blah', 'bleh', 'bless', 'blessed', 'blk', 'blog', 'blogcatalog', 'blogger', 'bloggers', 'blogging', 'blogs', 'bloody', 'book', 'bored', 'boring', 'bottle', 'bottles', 'bought', 'box', 'boxes', 'boy', 'boys', 'break', 'breakfast', 'breakfasts', 'bright', 'bring', 'brings', 'bro', 'broke', 'broken', 'bros', 'brought', 'btw', 'build', 'builds', 'built', 'bus', 'buses', 'butter', 'buy', 'buys', 'bye', 'byebye', 'byee', 'call', 'called', 'calls', 'cancel', 'canceled', 'cancelled', 'cancels', 'candies', 'candy', 'car', 'career', 'careers', 'cars', 'catch', 'catches', 'caught', 'change', 'changed', 'changes', 'changing', 'channel', 'channels', 'cheap', 'check', 'checked', 'checks', 'chicken', 'chickens', 'chocolate', 'chocolates', 'choice', 'choices', 'class', 'classes', 'click', 'close', 'closed', 'closes', 'cloth', 'clothe', 'clothes', 'clutter', 'cluttered', 'cna', 'coffee', 'com', 'comeback', 'comment', 'commenting', 'comments', 'common', 'companies', 'company', 'complete', 'completed', 'completes', 'completing', 'conditions', 'condo', 'condominium', 'condominoums', 'condos', 'congrats', 'congratulation', 'congratulations', 'consecutive', 'consecutively', 'consult', 'consultant', 'consults', 'contact', 'contacted', 'contacts', 'contd', 'content', 'contents', 'continue', 'continues', 'conv', 'cookies', 'cos', 'cost', 'costs', 'couldn', 'couldnt', 'countries', 'country', 'couple', 'couples', 'course', 'courses', 'cove', 'coves', 'coz', 'crap', 'crappy', 'crazy', 'cream', 'create', 'created', 'creates', 'creats', 'crowded', 'cum', 'curnews', 'curr', 'customer', 'customers', 'cute', 'cuties', 'cuz', 'dad', 'daily', 'damn', 'dark', 'dat', 'date', 'dated', 'dates', 'day', 'days', 'de', 'dead', 'dear', 'death', 'december', 'depend', 'depended', 'depends', 'deposit', 'deposited', 'deposits', 'detail', 'details', 'didn', 'didnt', 'die', 'died', 'dies', 'diff', 'dinner', 'dinners', 'dis', 'distract', 'distracted', 'distracts', 'doc', 'docs', 'document', 'documents', 'doesn', 'doesnt', 'don', 'dont', 'door', 'doors', 'double', 'doubled', 'doubles', 'download', 'downloads', 'dr', 'dreamt', 'drs', 'due', 'dun', 'dunno', 'duper', 'earlier', 'earliest', 'early', 'earn', 'earned', 'earns', 'easier', 'easy', 'eat', 'eaten', 'eats', 'eh', 'ehh', 'email', 'emails', 'emo', 'emos', 'enable', 'enables', 'enabling', 'end', 'ends', 'eng', 'enter', 'entered', 'enters', 'esp', 'event', 'events', 'everyday', 'everywhere', 'exclude', 'excluded', 'excludes', 'excuse', 'excused', 'excuses', 'explode', 'exploded', 'explodes', 'eye', 'eyes', 'fadein', 'fail', 'failed', 'fails', 'fake', 'fall', 'falls', 'false', 'families', 'family', 'famous', 'fast', 'faster', 'fastest', 'fat', 'favorite', 'favorited', 'favorites', 'favourite', 'favourites', 'featured', 'february', 'feed', 'feeds', 'feel', 'feeling', 'feels', 'fell', 'felt', 'female', 'females', 'ffs', 'finally', 'find', 'finds', 'finish', 'finished', 'flat', 'flats', 'flight', 'flights', 'fml', 'follow', 'followed', 'follows', 'food', 'form', 'formed', 'forming', 'forms', 'found', 'free', 'friday', 'friend', 'friends', 'fries', 'frm', 'fruit', 'fruits', 'ftl', 'ftw', 'fu', 'fuck', 'fucks', 'full', 'fully', 'fun', 'funny', 'furnish', 'furnished', 'future', 'fwah', 'g2g', 'gajshost', 'gave', 'gd', 'geez', 'gg', 'gift', 'gifted', 'gifts', 'gigs', 'gimme', 'girl', 'girls', 'give', 'giveaway', 'giveaways', 'given', 'gives', 'gonna', 'good', 'goodbye', 'goodnight', 'got', 'gotta', 'grats', 'gratz', 'great', 'greats', 'gtfo', 'gtg', 'guess', 'guessing', 'guy', 'guys', 'haa', 'haha', 'hahaha', 'hair', 'hairs', 'hand', 'hands', 'happen', 'happened', 'happens', 'hard', 'harder', 'hardest', 'hasn', 'hasnt', 'hate', 'hated', 'hates', 'hav', 'haven', 'havent', 'hdb', 'hear', 'heard', 'hears', 'heart', 'hee', 'heh', 'hehe', 'hehehe', 'hello', 'hey', 'hi', 'highest', 'hmm', 'ho', 'hohoho', 'holiday', 'holidays', 'home', 'homework', 'homeworks', 'hope', 'hopefully', 'hoping', 'host', 'hosted', 'hosts', 'hot', 'hour', 'hours', 'http', 'https', 'huge', 'huh', 'huhu', 'huhuhu', 'hurt', 'hurts', 'idea', 'ideas', 'idiot', 'idiots', 'idk', 'iirc', 'im', 'imho', 'imo', 'important', 'indicate', 'indicated', 'indicates', 'indicating', 'info', 'information', 'ini', 'install', 'installation', 'installations', 'installs', 'interact', 'interacted', 'interacting', 'interaction', 'interactions', 'interacts', 'interested', 'interesting', 'internet', 'introduction', 'introductions', 'involve', 'involved', 'involves', 'irl', 'ish', 'isn', 'isnt', 'issue', 'issued', 'issues', 'item', 'items', 'january', 'jk', 'job', 'jobs', 'july', 'june', 'jus', 'just', 'justwit', 'juz', 'key', 'keys', 'kid', 'kids', 'kill', 'killed', 'kills', 'kinda', 'kthx', 'kthxbai', 'kyou', 'laa', 'laaa', 'lah', 'lanuch', 'last', 'late', 'later', 'latest', 'laugh', 'laughed', 'laughs', 'launched', 'launches', 'lawl', 'learn', 'learned', 'learns', 'leavg', 'leh', 'lend', 'lender', 'lenders', 'lfg', 'lfm', 'life', 'light', 'lightly', 'lights', 'like', 'liked', 'likes', 'line', 'lined', 'lines', 'link', 'links', 'listed', 'listen', 'listened', 'listening', 'listens', 'live', 'lived', 'lives', 'll', 'lmao', 'lmfao', 'lnks', 'loaned', 'local', 'locate', 'located', 'locates', 'location', 'locations', 'lock', 'locked', 'locks', 'lol', 'lols', 'look', 'looked', 'looking', 'looks', 'lose', 'loss', 'lot', 'lotsa', 'lotta', 'love', 'loved', 'loves', 'lowest', 'ltd', 'lunch', 'lunches', 'luv', 'ly', 'macdailynews', 'mad', 'made', 'mailed', 'main', 'make', 'makes', 'male', 'males', 'man', 'manage', 'managed', 'manages', 'march', 'may', 'mean', 'means', 'media', 'medias', 'mee', 'meet', 'meets', 'meh', 'men', 'mens', 'mention', 'mentioned', 'mentions', 'menu', 'menus', 'merry', 'message', 'messages', 'met', 'million', 'millions', 'min', 'mine', 'mini', 'mins', 'minute', 'minutes', 'miss', 'missed', 'misses', 'mix', 'mixed', 'mixes', 'mom', 'monday', 'money', 'month', 'monthly', 'months', 'morning', 'movie', 'movies', 'mph', 'mrt', 'msg', 'msgs', 'muahahahahaha', 'mum', 'mushroom', 'music', 'musics', 'named', 'national', 'nb', 'neato', 'negotiable', 'net', 'neways', 'newly', 'news', 'ni', 'nice', 'night', 'nite', 'nom', 'noodle', 'noodles', 'noscript', 'notes', 'notice', 'notices', 'noticing', 'notified', 'notifies', 'notify', 'november', 'now', 'nowadays', 'nvr', 'nw', 'obvious', 'obviously', 'occur', 'occured', 'occurs', 'october', 'office', 'offices', 'ohayo', 'old', 'omfg', 'omfgwtf', 'omg', 'omgwtfbbq', 'omw', 'online', 'only', 'open', 'opened', 'opens', 'opportunities', 'opportunity', 'order', 'ordered', 'ordering', 'orders', 'org', 'organize', 'organized', 'organizes', 'pack', 'packed', 'packs', 'page', 'pages', 'paid', 'pain', 'painful', 'painless', 'pains', 'pair', 'parent', 'parents', 'park', 'parks', 'passed', 'past', 'pay', 'pays', 'people', 'person', 'pf', 'phew', 'phone', 'phones', 'photo', 'photos', 'pic', 'pick', 'picked', 'picks', 'pics', 'pictures', 'pig', 'pigs', 'pissed', 'place', 'places', 'play', 'played', 'plays', 'pls', 'plz', 'plzz', 'pm', 'pmsing', 'post', 'posted', 'posts', 'powerful', 'ppl', 'pre', 'prefer', 'present', 'presentation', 'presentations', 'presented', 'presents', 'pretty', 'preview', 'previewed', 'previews', 'previous', 'price', 'priced', 'prices', 'primary', 'private', 'pro', 'problem', 'problems', 'produce', 'produced', 'produces', 'product', 'production', 'productions', 'products', 'profile', 'profiles', 'programs', 'project', 'projects', 'prolly', 'prosperous', 'provide', 'provided', 'provides', 'psd', 'pte', 'public', 'purchase', 'purchased', 'purchases', 'pwm', 'pwned', 'qfmft', 'qft', 'quality', 'quick', 'ran', 'rate', 'rated', 'rawr', 'rawrr', 'reader', 'readers', 'real', 'realise', 'realised', 'realises', 'realize', 'realized', 'realizes', 'receive', 'received', 'receives', 'recent', 'recently', 'recommend', 'recommended', 'recommends', 'recover', 'recovered', 'recovers', 'refuse', 'refused', 'refuses', 'regular', 'rejoice', 'relate', 'related', 'relates', 'remember', 'remembered', 'remembers', 'remind', 'reminded', 'reminds', 'remove', 'removed', 'removes', 'rename', 'renamed', 'renames', 'rent', 'rental', 'rented', 'rents', 'replace', 'replaced', 'replaces', 'replied', 'replies', 'reply', 'report', 'reported', 'reports', 'request', 'requested', 'requests', 'require', 'required', 'requires', 'resort', 'resorts', 'result', 'resulted', 'results', 'return', 'returned', 'returns', 'retweet', 'retweets', 'review', 'reviewed', 'reviews', 'rice', 'right', 'rightaway', 'road', 'roads', 'rocks', 'rofl', 'roflmao', 'role', 'roles', 'room', 'rooms', 'rss', 'rt', 'run', 'runs', 'safe', 'safety', 'sale', 'sales', 'sang', 'saturday', 'save', 'saved', 'saves', 'scratch', 'scratched', 'scratches', 'screen', 'screened', 'screens', 'screwed', 'screws', 'search', 'searched', 'searches', 'season', 'seasons', 'sec', 'secondary', 'secs', 'seem', 'seemed', 'seems', 'select', 'selected', 'selecting', 'selects', 'sell', 'sells', 'send', 'sends', 'sent', 'september', 'serve', 'served', 'serves', 'service', 'services', 'set', 'sets', 'settle', 'settled', 'settles', 'sg', 'sgd', 'sgreinfo', 'share', 'shared', 'shares', 'shd', 'shit', 'shits', 'shitz', 'shld', 'shoe', 'shoes', 'shop', 'shops', 'shortest', 'shouldn', 'shouldnt', 'show', 'showed', 'shown', 'shows', 'shudder', 'sick', 'sicks', 'side', 'sides', 'signed', 'significant', 'significantly', 'similar', 'similars', 'sing', 'singapore', 'singaporestd', 'single', 'singled', 'singles', 'sings', 'site', 'sites', 'skin', 'sleep', 'sleeps', 'slept', 'slight', 'slightly', 'slipped', 'slow', 'small', 'sms', 'soba', 'soft', 'sold', 'somemore', 'son', 'sons', 'soon', 'sore', 'sores', 'sound', 'sounded', 'sounds', 'soup', 'soups', 'source', 'sources', 'special', 'specials', 'specific', 'specifically', 'spend', 'spending', 'spends', 'spent', 'spot', 'spots', 'sq', 'sqft', 'sqm', 'srsly', 'start', 'started', 'starts', 'stay', 'stayed', 'stays', 'stfu', 'stks', 'stop', 'stopped', 'stops', 'stories', 'story', 'strong', 'student', 'students', 'studied', 'studies', 'study', 'stuff', 'stupid', 'stupids', 'su', 'suck', 'sucked', 'sucks', 'suckz', 'sue', 'sued', 'sues', 'sunday', 'sundays', 'sung', 'support', 'supported', 'supporting', 'supports', 'sux', 'sweet', 'swf', 'sync', 'take', 'takes', 'taking', 'talk', 'talked', 'talks', 'tallest', 'target', 'targets', 'tart', 'tat', 'taught', 'tbh', 'tbl', 'tea', 'teach', 'teacher', 'teachers', 'teaches', 'teehee', 'tel', 'tells', 'tgif', 'thank', 'thanks', 'thanky', 'theme', 'themes', 'thing', 'things', 'think', 'thinks', 'thk', 'thks', 'thought', 'throat', 'throats', 'tht', 'thursday', 'ticket', 'tickets', 'time', 'tips', 'tired', 'tis', 'tm', 'tmr', 'today', 'toilet', 'toilets', 'told', 'tomorrow', 'tonight', 'took', 'total', 'totals', 'treat', 'treated', 'treats', 'tree', 'trees', 'true', 'tsk', 'ttyl', 'tuesday', 'turn', 'turned', 'turns', 'tweet', 'tweeting', 'tweets', 'twittering', 'ty', 'tym', 'tyme', 'type', 'typed', 'types', 'tyty', 'tyvm', 'um', 'umm', 'unit', 'units', 'update', 'updated', 'updates', 'upgrade', 'upgraded', 'upgrades', 'upload', 'uploaded', 'uploads', 'url', 'urls', 'usb', 'usd', 'user', 'users', 'va', 'valid', 'valids', 'var', 'vc', 've', 'version', 'versions', 'video', 'videos', 'visit', 'visited', 'visits', 'viv', 'vn', 'vote', 'voted', 'votes', 'w00t', 'wa', 'wadever', 'wah', 'wait', 'waited', 'waiting', 'waits', 'wanna', 'want', 'wanted', 'wants', 'wasn', 'wasnt', 'wassup', 'wat', 'watch', 'watcha', 'watched', 'watches', 'watching', 'wateva', 'watever', 'watnot', 'wats', 'wayy', 'wb', 'web', 'website', 'websites', 'wednesday', 'week', 'weekly', 'weeks', 'weird', 'weren', 'werent', 'whaha', 'wham', 'whammy', 'whaow', 'whatcha', 'whatev', 'whateva', 'whatevar', 'whatever', 'whatnot', 'whats', 'whatsoever', 'whatz', 'whee', 'whenz', 'whey', 'white', 'whore', 'whores', 'whoring', 'win', 'wins', 'wish', 'wished', 'wishes', 'wo', 'woah', 'woh', 'woman', 'women', 'womens', 'won', 'wonder', 'wondered', 'wondering', 'wonders', 'wooohooo', 'woot', 'word', 'words', 'work', 'working', 'works', 'world', 'worlds', 'worse', 'worst', 'wow', 'write', 'writes', 'written', 'wrong', 'wrote', 'wrt', 'wtb', 'wtf', 'wth', 'wts', 'wtt', 'www', 'xs', 'ya', 'yaah', 'yah', 'yahh', 'yahoocurrency', 'yall', 'yar', 'yay', 'yea', 'yeah', 'yeahh', 'year', 'yearly', 'years', 'yeh', 'yesterday', 'yhoo', 'ymmv', 'young', 'youre', 'yr', 'yum', 'yummy', 'yumyum', 'yw', 'zomg', 'zz', 'zzz'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 8867), ('the', 7240), ('my', 5434), ('it', 4361), ('and', 4260), ('is', 3640), ('in', 3239), ('you', 2927), ('for', 2736), ('of', 2544), ('me', 2510), ('so', 2504), ('on', 2352), ('but', 2305), ('that', 2304), ('have', 2288), ('not', 2026), ('just', 1813), ('at', 1794), ('be', 1692), ('was', 1645), ('this', 1530), ('now', 1515), ('no', 1481), ('can', 1439)]\n"
     ]
    }
   ],
   "source": [
    "#get sample stopwords file\n",
    "stopwords=get_stop_words(\"twitter-stopwords.txt\")\n",
    "#processed negative tweets text\n",
    "docs=negativetweets['processed_text'].tolist()\n",
    "most_common_negative=get_top_n_words(docs,25,stopwords)\n",
    "print(most_common_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleframe= [positivetweets,negativetweets]\n",
    "df = pd.concat(sampleframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using a machine learning classifier such as a Naive Bayes Classifier. Select 15,000 tweets randomly from each of the 3 categories (0 = negative, 2 = neutral, 4 = positive)to build your training dataset. Select 3000 tweets randomly from the 3 categories to build your test set. Make sure there is no overlap between the training and test sets. Use the training data to train your model and the test data to evaluate the performance of your model. Report the Precision, Recall, and F-1 score of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),#tokenizing text\n",
    "                     ('tfidf', TfidfTransformer()),#extract term frequencey times\n",
    "                     ('clf', MultinomialNB())])# multinomial Naives Bayes\n",
    "tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': [1, 1e-1, 1e-2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['processed_text']\n",
    "labels = df['polarity']\n",
    "categories = [0,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=40)\n",
    "text_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.754"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.721     0.829     0.771      6743\n",
      "           4      0.799     0.679     0.734      6757\n",
      "\n",
      "   micro avg      0.754     0.754     0.754     13500\n",
      "   macro avg      0.760     0.754     0.753     13500\n",
      "weighted avg      0.760     0.754     0.753     13500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test,predicted,digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Stop Word Library\n",
    "#read in stop list words\n",
    "#https://sites.google.com/site/kevinbouge/stopwords-lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(max_df=0.85,stop_words=stopwords)),#tokenizing text\n",
    "                     ('tfidf', TfidfTransformer()),#extract term frequencey times\n",
    "                     ('clf', MultinomialNB())])# multinomial Naives Bayes\n",
    "tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': [1, 1e-1, 1e-2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['processed_text']\n",
    "labels = df['polarity']\n",
    "categories = [0,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wingo\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aahh', 'aarrgghh', 'abt', 'accent', 'accented', 'accents', 'account', 'accounts', 'acne', 'activities', 'activity', 'ad', 'add', 'added', 'adding', 'adds', 'admission', 'admissions', 'ads', 'afaik', 'affiliate', 'affiliates', 'affirmation', 'affirmations', 'aft', 'afternoon', 'ago', 'ahead', 'ain', 'aint', 'aircon', 'album', 'albums', 'allergies', 'allergy', 'allow', 'allowed', 'allows', 'alot', 'am', 'angry', 'announcement', 'announcements', 'annoy', 'annoyed', 'annoys', 'anycase', 'anymore', 'app', 'apparently', 'approve', 'approved', 'approves', 'apps', 'april', 'area', 'areas', 'argh', 'arrive', 'arrived', 'arrives', 'article', 'articles', 'asia', 'asian', 'ask', 'asked', 'asks', 'ass', 'asses', 'ate', 'attempt', 'attempting', 'attempts', 'attend', 'attended', 'attends', 'august', 'auto', 'autoindustry', 'awesome', 'babeh', 'babies', 'baby', 'back', 'backed', 'bad', 'bag', 'bags', 'bai', 'balance', 'bank', 'banks', 'based', 'bcos', 'bcoz', 'bday', 'bed', 'bedroom', 'belong', 'belonged', 'belongs', 'big', 'bigger', 'biggest', 'billion', 'billons', 'birthday', 'birthdays', 'bit', 'biz', 'blah', 'bleh', 'bless', 'blessed', 'blk', 'blog', 'blogcatalog', 'blogger', 'bloggers', 'blogging', 'blogs', 'bloody', 'book', 'bored', 'boring', 'bottle', 'bottles', 'bought', 'box', 'boxes', 'boy', 'boys', 'break', 'breakfast', 'breakfasts', 'bright', 'bring', 'brings', 'bro', 'broke', 'broken', 'bros', 'brought', 'btw', 'build', 'builds', 'built', 'bus', 'buses', 'butter', 'buy', 'buys', 'bye', 'byebye', 'byee', 'call', 'called', 'calls', 'cancel', 'canceled', 'cancelled', 'cancels', 'candies', 'candy', 'car', 'career', 'careers', 'cars', 'catch', 'catches', 'caught', 'change', 'changed', 'changes', 'changing', 'channel', 'channels', 'cheap', 'check', 'checked', 'checks', 'chicken', 'chickens', 'chocolate', 'chocolates', 'choice', 'choices', 'class', 'classes', 'click', 'close', 'closed', 'closes', 'cloth', 'clothe', 'clothes', 'clutter', 'cluttered', 'cna', 'coffee', 'com', 'comeback', 'comment', 'commenting', 'comments', 'common', 'companies', 'company', 'complete', 'completed', 'completes', 'completing', 'conditions', 'condo', 'condominium', 'condominoums', 'condos', 'congrats', 'congratulation', 'congratulations', 'consecutive', 'consecutively', 'consult', 'consultant', 'consults', 'contact', 'contacted', 'contacts', 'contd', 'content', 'contents', 'continue', 'continues', 'conv', 'cookies', 'cos', 'cost', 'costs', 'couldn', 'couldnt', 'countries', 'country', 'couple', 'couples', 'course', 'courses', 'cove', 'coves', 'coz', 'crap', 'crappy', 'crazy', 'cream', 'create', 'created', 'creates', 'creats', 'crowded', 'cum', 'curnews', 'curr', 'customer', 'customers', 'cute', 'cuties', 'cuz', 'dad', 'daily', 'damn', 'dark', 'dat', 'date', 'dated', 'dates', 'day', 'days', 'de', 'dead', 'dear', 'death', 'december', 'depend', 'depended', 'depends', 'deposit', 'deposited', 'deposits', 'detail', 'details', 'didn', 'didnt', 'die', 'died', 'dies', 'diff', 'dinner', 'dinners', 'dis', 'distract', 'distracted', 'distracts', 'doc', 'docs', 'document', 'documents', 'doesn', 'doesnt', 'don', 'dont', 'door', 'doors', 'double', 'doubled', 'doubles', 'download', 'downloads', 'dr', 'dreamt', 'drs', 'due', 'dun', 'dunno', 'duper', 'earlier', 'earliest', 'early', 'earn', 'earned', 'earns', 'easier', 'easy', 'eat', 'eaten', 'eats', 'eh', 'ehh', 'email', 'emails', 'emo', 'emos', 'enable', 'enables', 'enabling', 'end', 'ends', 'eng', 'enter', 'entered', 'enters', 'esp', 'event', 'events', 'everyday', 'everywhere', 'exclude', 'excluded', 'excludes', 'excuse', 'excused', 'excuses', 'explode', 'exploded', 'explodes', 'eye', 'eyes', 'fadein', 'fail', 'failed', 'fails', 'fake', 'fall', 'falls', 'false', 'families', 'family', 'famous', 'fast', 'faster', 'fastest', 'fat', 'favorite', 'favorited', 'favorites', 'favourite', 'favourites', 'featured', 'february', 'feed', 'feeds', 'feel', 'feeling', 'feels', 'fell', 'felt', 'female', 'females', 'ffs', 'finally', 'find', 'finds', 'finish', 'finished', 'flat', 'flats', 'flight', 'flights', 'fml', 'follow', 'followed', 'follows', 'food', 'form', 'formed', 'forming', 'forms', 'found', 'free', 'friday', 'friend', 'friends', 'fries', 'frm', 'fruit', 'fruits', 'ftl', 'ftw', 'fu', 'fuck', 'fucks', 'full', 'fully', 'fun', 'funny', 'furnish', 'furnished', 'future', 'fwah', 'g2g', 'gajshost', 'gave', 'gd', 'geez', 'gg', 'gift', 'gifted', 'gifts', 'gigs', 'gimme', 'girl', 'girls', 'give', 'giveaway', 'giveaways', 'given', 'gives', 'gonna', 'good', 'goodbye', 'goodnight', 'got', 'gotta', 'grats', 'gratz', 'great', 'greats', 'gtfo', 'gtg', 'guess', 'guessing', 'guy', 'guys', 'haa', 'haha', 'hahaha', 'hair', 'hairs', 'hand', 'hands', 'happen', 'happened', 'happens', 'hard', 'harder', 'hardest', 'hasn', 'hasnt', 'hate', 'hated', 'hates', 'hav', 'haven', 'havent', 'hdb', 'hear', 'heard', 'hears', 'heart', 'hee', 'heh', 'hehe', 'hehehe', 'hello', 'hey', 'hi', 'highest', 'hmm', 'ho', 'hohoho', 'holiday', 'holidays', 'home', 'homework', 'homeworks', 'hope', 'hopefully', 'hoping', 'host', 'hosted', 'hosts', 'hot', 'hour', 'hours', 'http', 'https', 'huge', 'huh', 'huhu', 'huhuhu', 'hurt', 'hurts', 'idea', 'ideas', 'idiot', 'idiots', 'idk', 'iirc', 'im', 'imho', 'imo', 'important', 'indicate', 'indicated', 'indicates', 'indicating', 'info', 'information', 'ini', 'install', 'installation', 'installations', 'installs', 'interact', 'interacted', 'interacting', 'interaction', 'interactions', 'interacts', 'interested', 'interesting', 'internet', 'introduction', 'introductions', 'involve', 'involved', 'involves', 'irl', 'ish', 'isn', 'isnt', 'issue', 'issued', 'issues', 'item', 'items', 'january', 'jk', 'job', 'jobs', 'july', 'june', 'jus', 'just', 'justwit', 'juz', 'key', 'keys', 'kid', 'kids', 'kill', 'killed', 'kills', 'kinda', 'kthx', 'kthxbai', 'kyou', 'laa', 'laaa', 'lah', 'lanuch', 'last', 'late', 'later', 'latest', 'laugh', 'laughed', 'laughs', 'launched', 'launches', 'lawl', 'learn', 'learned', 'learns', 'leavg', 'leh', 'lend', 'lender', 'lenders', 'lfg', 'lfm', 'life', 'light', 'lightly', 'lights', 'like', 'liked', 'likes', 'line', 'lined', 'lines', 'link', 'links', 'listed', 'listen', 'listened', 'listening', 'listens', 'live', 'lived', 'lives', 'll', 'lmao', 'lmfao', 'lnks', 'loaned', 'local', 'locate', 'located', 'locates', 'location', 'locations', 'lock', 'locked', 'locks', 'lol', 'lols', 'look', 'looked', 'looking', 'looks', 'lose', 'loss', 'lot', 'lotsa', 'lotta', 'love', 'loved', 'loves', 'lowest', 'ltd', 'lunch', 'lunches', 'luv', 'ly', 'macdailynews', 'mad', 'made', 'mailed', 'main', 'make', 'makes', 'male', 'males', 'man', 'manage', 'managed', 'manages', 'march', 'may', 'mean', 'means', 'media', 'medias', 'mee', 'meet', 'meets', 'meh', 'men', 'mens', 'mention', 'mentioned', 'mentions', 'menu', 'menus', 'merry', 'message', 'messages', 'met', 'million', 'millions', 'min', 'mine', 'mini', 'mins', 'minute', 'minutes', 'miss', 'missed', 'misses', 'mix', 'mixed', 'mixes', 'mom', 'monday', 'money', 'month', 'monthly', 'months', 'morning', 'movie', 'movies', 'mph', 'mrt', 'msg', 'msgs', 'muahahahahaha', 'mum', 'mushroom', 'music', 'musics', 'named', 'national', 'nb', 'neato', 'negotiable', 'net', 'neways', 'newly', 'news', 'ni', 'nice', 'night', 'nite', 'nom', 'noodle', 'noodles', 'noscript', 'notes', 'notice', 'notices', 'noticing', 'notified', 'notifies', 'notify', 'november', 'now', 'nowadays', 'nvr', 'nw', 'obvious', 'obviously', 'occur', 'occured', 'occurs', 'october', 'office', 'offices', 'ohayo', 'old', 'omfg', 'omfgwtf', 'omg', 'omgwtfbbq', 'omw', 'online', 'only', 'open', 'opened', 'opens', 'opportunities', 'opportunity', 'order', 'ordered', 'ordering', 'orders', 'org', 'organize', 'organized', 'organizes', 'pack', 'packed', 'packs', 'page', 'pages', 'paid', 'pain', 'painful', 'painless', 'pains', 'pair', 'parent', 'parents', 'park', 'parks', 'passed', 'past', 'pay', 'pays', 'people', 'person', 'pf', 'phew', 'phone', 'phones', 'photo', 'photos', 'pic', 'pick', 'picked', 'picks', 'pics', 'pictures', 'pig', 'pigs', 'pissed', 'place', 'places', 'play', 'played', 'plays', 'pls', 'plz', 'plzz', 'pm', 'pmsing', 'post', 'posted', 'posts', 'powerful', 'ppl', 'pre', 'prefer', 'present', 'presentation', 'presentations', 'presented', 'presents', 'pretty', 'preview', 'previewed', 'previews', 'previous', 'price', 'priced', 'prices', 'primary', 'private', 'pro', 'problem', 'problems', 'produce', 'produced', 'produces', 'product', 'production', 'productions', 'products', 'profile', 'profiles', 'programs', 'project', 'projects', 'prolly', 'prosperous', 'provide', 'provided', 'provides', 'psd', 'pte', 'public', 'purchase', 'purchased', 'purchases', 'pwm', 'pwned', 'qfmft', 'qft', 'quality', 'quick', 'ran', 'rate', 'rated', 'rawr', 'rawrr', 'reader', 'readers', 'real', 'realise', 'realised', 'realises', 'realize', 'realized', 'realizes', 'receive', 'received', 'receives', 'recent', 'recently', 'recommend', 'recommended', 'recommends', 'recover', 'recovered', 'recovers', 'refuse', 'refused', 'refuses', 'regular', 'rejoice', 'relate', 'related', 'relates', 'remember', 'remembered', 'remembers', 'remind', 'reminded', 'reminds', 'remove', 'removed', 'removes', 'rename', 'renamed', 'renames', 'rent', 'rental', 'rented', 'rents', 'replace', 'replaced', 'replaces', 'replied', 'replies', 'reply', 'report', 'reported', 'reports', 'request', 'requested', 'requests', 'require', 'required', 'requires', 'resort', 'resorts', 'result', 'resulted', 'results', 'return', 'returned', 'returns', 'retweet', 'retweets', 'review', 'reviewed', 'reviews', 'rice', 'right', 'rightaway', 'road', 'roads', 'rocks', 'rofl', 'roflmao', 'role', 'roles', 'room', 'rooms', 'rss', 'rt', 'run', 'runs', 'safe', 'safety', 'sale', 'sales', 'sang', 'saturday', 'save', 'saved', 'saves', 'scratch', 'scratched', 'scratches', 'screen', 'screened', 'screens', 'screwed', 'screws', 'search', 'searched', 'searches', 'season', 'seasons', 'sec', 'secondary', 'secs', 'seem', 'seemed', 'seems', 'select', 'selected', 'selecting', 'selects', 'sell', 'sells', 'send', 'sends', 'sent', 'september', 'serve', 'served', 'serves', 'service', 'services', 'set', 'sets', 'settle', 'settled', 'settles', 'sg', 'sgd', 'sgreinfo', 'share', 'shared', 'shares', 'shd', 'shit', 'shits', 'shitz', 'shld', 'shoe', 'shoes', 'shop', 'shops', 'shortest', 'shouldn', 'shouldnt', 'show', 'showed', 'shown', 'shows', 'shudder', 'sick', 'sicks', 'side', 'sides', 'signed', 'significant', 'significantly', 'similar', 'similars', 'sing', 'singapore', 'singaporestd', 'single', 'singled', 'singles', 'sings', 'site', 'sites', 'skin', 'sleep', 'sleeps', 'slept', 'slight', 'slightly', 'slipped', 'slow', 'small', 'sms', 'soba', 'soft', 'sold', 'somemore', 'son', 'sons', 'soon', 'sore', 'sores', 'sound', 'sounded', 'sounds', 'soup', 'soups', 'source', 'sources', 'special', 'specials', 'specific', 'specifically', 'spend', 'spending', 'spends', 'spent', 'spot', 'spots', 'sq', 'sqft', 'sqm', 'srsly', 'start', 'started', 'starts', 'stay', 'stayed', 'stays', 'stfu', 'stks', 'stop', 'stopped', 'stops', 'stories', 'story', 'strong', 'student', 'students', 'studied', 'studies', 'study', 'stuff', 'stupid', 'stupids', 'su', 'suck', 'sucked', 'sucks', 'suckz', 'sue', 'sued', 'sues', 'sunday', 'sundays', 'sung', 'support', 'supported', 'supporting', 'supports', 'sux', 'sweet', 'swf', 'sync', 'take', 'takes', 'taking', 'talk', 'talked', 'talks', 'tallest', 'target', 'targets', 'tart', 'tat', 'taught', 'tbh', 'tbl', 'tea', 'teach', 'teacher', 'teachers', 'teaches', 'teehee', 'tel', 'tells', 'tgif', 'thank', 'thanks', 'thanky', 'theme', 'themes', 'thing', 'things', 'think', 'thinks', 'thk', 'thks', 'thought', 'throat', 'throats', 'tht', 'thursday', 'ticket', 'tickets', 'time', 'tips', 'tired', 'tis', 'tm', 'tmr', 'today', 'toilet', 'toilets', 'told', 'tomorrow', 'tonight', 'took', 'total', 'totals', 'treat', 'treated', 'treats', 'tree', 'trees', 'true', 'tsk', 'ttyl', 'tuesday', 'turn', 'turned', 'turns', 'tweet', 'tweeting', 'tweets', 'twittering', 'ty', 'tym', 'tyme', 'type', 'typed', 'types', 'tyty', 'tyvm', 'um', 'umm', 'unit', 'units', 'update', 'updated', 'updates', 'upgrade', 'upgraded', 'upgrades', 'upload', 'uploaded', 'uploads', 'url', 'urls', 'usb', 'usd', 'user', 'users', 'va', 'valid', 'valids', 'var', 'vc', 've', 'version', 'versions', 'video', 'videos', 'visit', 'visited', 'visits', 'viv', 'vn', 'vote', 'voted', 'votes', 'w00t', 'wa', 'wadever', 'wah', 'wait', 'waited', 'waiting', 'waits', 'wanna', 'want', 'wanted', 'wants', 'wasn', 'wasnt', 'wassup', 'wat', 'watch', 'watcha', 'watched', 'watches', 'watching', 'wateva', 'watever', 'watnot', 'wats', 'wayy', 'wb', 'web', 'website', 'websites', 'wednesday', 'week', 'weekly', 'weeks', 'weird', 'weren', 'werent', 'whaha', 'wham', 'whammy', 'whaow', 'whatcha', 'whatev', 'whateva', 'whatevar', 'whatever', 'whatnot', 'whats', 'whatsoever', 'whatz', 'whee', 'whenz', 'whey', 'white', 'whore', 'whores', 'whoring', 'win', 'wins', 'wish', 'wished', 'wishes', 'wo', 'woah', 'woh', 'woman', 'women', 'womens', 'won', 'wonder', 'wondered', 'wondering', 'wonders', 'wooohooo', 'woot', 'word', 'words', 'work', 'working', 'works', 'world', 'worlds', 'worse', 'worst', 'wow', 'write', 'writes', 'written', 'wrong', 'wrote', 'wrt', 'wtb', 'wtf', 'wth', 'wts', 'wtt', 'www', 'xs', 'ya', 'yaah', 'yah', 'yahh', 'yahoocurrency', 'yall', 'yar', 'yay', 'yea', 'yeah', 'yeahh', 'year', 'yearly', 'years', 'yeh', 'yesterday', 'yhoo', 'ymmv', 'young', 'youre', 'yr', 'yum', 'yummy', 'yumyum', 'yw', 'zomg', 'zz', 'zzz'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.85, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=frozenset(...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "text_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7499259259259259"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.714     0.826     0.766      6688\n",
      "           4      0.798     0.675     0.732      6812\n",
      "\n",
      "   micro avg      0.750     0.750     0.750     13500\n",
      "   macro avg      0.756     0.751     0.749     13500\n",
      "weighted avg      0.756     0.750     0.749     13500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test,predicted,digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
